# IM Chat System 中的 ZSet、Kafka、Feed 流与 WebSocket 技术详解

> 本文从**基础概念**讲起，结合本项目的具体实现，系统说明 Redis ZSet、Kafka、Feed 流设计和 WebSocket/STOMP 在即时通讯场景中的作用与设计取舍。你可以把它当作：
>
> - 面试时讲解项目技术亮点的“底稿”；
> - 写博客或 README 的基础版本；
> - 自己复习分布式消息、缓存和长连接的笔记。

---

## 1. Redis ZSet：有序集合与热门榜

### 1.1 ZSet 是什么？

Redis 提供的有序集合（Sorted Set，简称 **ZSet**）有几个核心特性：

- 每个元素都是一个唯一的 `member`，同时带一个 `score`（双精度浮点数）。
- Redis 会按 `score` 自动排序，支持：
  - `ZADD key score member`：插入/更新元素；
  - `ZRANGE key start stop [WITHSCORES]`：按分值**从小到大**取；
  - `ZREVRANGE key start stop [WITHSCORES]`：按分值**从大到小**取（常用于排行榜）；
  - `ZREM key member`：移除元素；
  - `ZINCRBY key increment member`：给某个元素的 score 增减。
- 底层使用**跳表 + 压缩列表**等结构，使插入/查询/范围查询在 log 级别时间复杂度内完成，非常适合做排行榜、延时任务、优先级队列等。

简单理解：**ZSet = HashMap(member → score) + 排序好的有序链表**。

### 1.2 为什么用 ZSet 做热门帖子？

在 IM 广场场景里，我们希望：

- 按帖子热度（点赞数、收藏数、评论数、发布时间衰减等）排序；
- 高并发读写：用户行为频繁（不断点赞/取消点赞/收藏/评论）；
- 排行榜通常只需要前 N 条（如前 50、前 100）；
- 实时性要求较高，希望用户刷到的“热门”接近实时。

如果只用 MySQL：

- 每次都 `ORDER BY hot_score DESC LIMIT 50`，随着数据量增长会越来越慢；
- 高并发下对单库排序压力很大。

用 Redis ZSet 的优势：

- 读：`ZREVRANGE hot:posts 0 49 WITHSCORES` 即可拿到前 50；
- 写：每次点赞/收藏/评论时，只需 `ZINCRBY` 一次即可；
- 排序和数据都在内存中，访问速度极快，天然适合作为“热点数据层”。

### 1.3 本项目中 ZSet 的设计与用法

在本项目的 `im-square-service` 中，我们用 ZSet 维护**广场热门帖子榜单**，大致设计：

- **Key 设计**：
  - 例如：`square:post:hot:zset`。
  - 也可以根据时间窗口区分，如 `square:post:hot:zset:3d`。

- **Member 设计**：
  - 使用 `postId` 作为 member（长整型 ID 字符串）。

- **Score 设计（热度分）**：
  - 基础思想：点赞、收藏、评论等行为权重不同：
    - 点赞：+1 分；
    - 收藏：+2 分；
    - 评论：+3 分；
  - 同时可以考虑时间衰减（新帖更容易上榜）：
    - 例如将 `score = base_score + time_factor`，time_factor 和发布时间（如“距现在多少小时”）相关；
  - 在当前实现中，至少会对点赞/收藏/评论等行为进行累加调整，把 ZSet 作为“热度计分板”。

- **写入逻辑**（示意）：
  - 用户点赞帖子：
    - 数据库表 `square_like` 新增记录；
    - 帖子 `like_count` 自增；
    - 调用 Redis：`ZINCRBY square:post:hot:zset 1 postId`；
  - 用户收藏帖子：
    - 自增 `favorite_count`；
    - `ZINCRBY square:post:hot:zset 2 postId`；
  - 用户评论帖子：
    - 自增 `comment_count`；
    - `ZINCRBY square:post:hot:zset 3 postId`；

- **读取热门榜**：
  - `GET /api/square/posts/hot` 接口：
    - 优先从 Redis ZSet 里 `ZREVRANGE` 出前 N 个 `postId`；
    - 然后批量查询数据库获取帖子详情（标题/内容/媒体等）；
    - 若 Redis 异常或数据缺失，则回退为 MySQL 排序查询。

- **容量与过期控制**：
  - 只保留最近若干天（如 3 天）内的帖子：
    - 可以定时执行 `ZREMRANGEBYSCORE`，根据时间做裁剪；
  - 控制 ZSet 长度（比如最多 5000~10000 条）：
    - 使用 `ZREMRANGEBYRANK` 只保留前 N 条，防止无界增长。

**效果**：

- 热门榜接口基本不打数据库，只在聚合详情时批量查一次；
- 点赞/收藏/评论这些动作对 Redis 的写入非常轻量；
- 排序与热度计算都在内存中完成，即便帖子很多也能保持高性能。

---

## 2. Kafka：异步消息与解耦

### 2.1 消息队列与 Kafka 的基本概念

**为什么需要消息队列？**

- 请求链路太长，直接串行写库会导致响应慢；
- 瞬时高并发时，数据库顶不住写入压力；
- 多个子系统都要对同一事件做处理，需要一个“广播”机制解耦。

**Kafka 的关键概念：**

- **Topic**：主题，类似“消息分类”，如 `im-message-private`、`square-feed-events`。
- **Partition**：分区，一个 Topic 可以有多个分区，以提升并发消费能力；同一分区内部消息有序。
- **Producer**：生产者，负责写入消息到 Topic；
- **Consumer**：消费者，从 Topic 的 Partition 里读取消息；
- **Consumer Group**：消费者组，组内实例共享分区，每条消息只会被组内一个实例消费；
- **Offset**：位移，记录消费进度，Kafka 通过 Offset 来保证“至少一次投递”。

典型的使用模式：

- 上游服务把事件写入 Kafka Topic（写入快，磁盘顺序写）；
- 下游一个或多个服务异步消费，根据事件做各自逻辑；
- 即使部分消费者挂掉，也不会影响上游请求链路。

### 2.2 本项目中 Kafka 的场景

本项目里 Kafka 主要用于两个方向：

1. **即时聊天消息的异步持久化**（核心亮点）；
2. **广场 Feed / 通知相关事件**的异步广播（用于解耦和扩展）。

#### 2.2.1 单聊消息异步持久化

传统做法：

- WebSocket 收到消息 → 马上写 MySQL → 成功后再推给对方 → 接口耗时完全依赖数据库。

本项目做法：**前端实时体验 + 后端异步落库** 分离：

1. 用户通过 WebSocket/STOMP 发送一条消息到 `im-message-service`；
2. 消息服务：
   - 使用雪花算法生成全局唯一 `messageId`；
   - 先将消息体写入 Redis：
     - `msg:detail:{messageId}` 存消息详情；
     - `msg:conv:{conversationKey}` 维护会话下消息 ID 列表；
   - 将“待持久化事件”写入 Kafka Topic：`im-message-private`；
   - 立即通过 WebSocket 把消息推给接收方（以及发送方本地回显），接口返回；
3. Kafka 消费者（仍在 `im-message-service` 里）异步消费：
   - 根据 `messageId` 从 Redis 取出消息详情；
   - 把消息写入 MySQL 的 `message` 表；
   - 更新 Redis 中这条消息的持久化状态（例如 `persisted = true`）；
   - 更新会话表：最近一条消息、未读数等；
4. 若消费或数据库写入失败，**补偿任务**会定期扫描 Redis 中状态为 `PENDING` 且超时的消息，重试写库或者打告警。

**优点：**

- 前端“发消息”的响应时间与数据库写入解耦，RT 近似 WebSocket 网络延迟；
- Kafka 以磁盘顺序写和批量消费的方式承载了写入峰值；
- 即便 DB 出现抖动，也不会直接影响发送路径，只会在后台写入阶段产生压力。

这一套是你在项目中非常值得讲的**高并发优化点和架构亮点**。

#### 2.2.2 广场 Feed / 通知事件

当用户在广场中：

- 发帖；
- 点赞别人帖子；
- 评论帖子；
- 关注用户；

这些行为本身要更新数据库（帖子、互动记录等），除此之外还会触发一系列“副作用”：

- 给对方发一条**广场通知**；
- 刷新对方的“我的广场消息”列表；
- 在未来可以用于：
  - 推荐系统（记录用户兴趣行为）；
  - 统计分析（PV/UV、每日互动量等）。

为了避免在核心写路径里把所有这些逻辑都串起来，本项目的思路是：

- 主流程：`im-square-service` 只负责写业务数据（帖子、点赞表等） + 简单的 Redis/ZSet 更新；
- 同时把行为事件写入 Kafka Topic（如 `square-feed-events`）；
- 由 `im-message-service` 或独立的“通知服务”订阅这些事件：
  - 生成广场通知记录；
  - 通过 WebSocket 推送给被通知人；
  - 后续可以新增其他消费者（埋点、报表系统等）而不影响现有服务。

这样就形成一个**事件驱动架构**的雏形。

---

## 3. Feed 流（时间线）在项目中的设计

### 3.1 Feed 流的基本概念

**Feed 流（Timeline）** 可以简单理解为“信息流”：

- 例如：微博首页 / 朋友圈 / B 站推荐列表 / Twitter Home Timeline；
- 每个用户看到的 Feed 流内容不一样，和他的关注关系、兴趣、行为密切相关。

常见的几种实现方式：

1. **Pull 模式（读时计算）**：
   - 用户打开首页时，再根据他关注的人查询这些人的内容，排序后返回；
   - 优点：写入简单；缺点：读压力大，实时聚合成本高。

2. **Push 模式（写时扇出）**：
   - 一个用户发帖时，直接把这条内容“推送”进所有粉丝的个人 Feed 列表里；
   - 优点：读很快；缺点：明星用户会产生巨量写放大（高 fanout）。

3. **混合模式**：
   - 对普通用户用 push，对大 V 用 pull 或冷启动算法；
   - 结合缓存、消息队列、推荐排序等。

在中小规模系统里，一般采用“较简单的 pull + 缓存 + 分页”的折中方案就足够。

### 3.2 本项目中的 Feed 流类型

本项目的大致 Feed 流有三类：

1. **广场“全部”时间线**（公共流）；
2. **广场“关注”时间线**（仅关注的人发的帖子）；
3. **广场消息通知流**（谁点赞/评论/关注了我）。

#### 3.2.1 公共时间线 + ZSet 热门榜

- 「全部」广场：
  - 接口：`GET /api/square/posts`；
  - 默认按时间倒序，使用 Redis 列表缓存分页结果；
  - 当用户带关键词、标签、是否有图/视频、可见范围、排序方式等筛选条件时，不再走缓存，而是使用 MyBatis 的动态 SQL 实时查询。
- 热门榜：
  - 接口：`GET /api/square/posts/hot`；
  - 完全由 Redis ZSet 驱动，属于一种 **按热度排序的公共 Feed 流**。

#### 3.2.2 关注时间线（Follow Feed）

- 数据模型：
  - `square_follow` / `user_follow` 表存储关注关系：`follower_id`、`followee_id`；
- 简化实现（偏 Pull）：
  - 打开“关注”Tab 时：
    - 查询出当前用户关注的人列表；
    - 在帖子表里 `WHERE author_id IN (关注的人)` 且可见性校验通过；
    - 按时间倒序或热度排序，分页返回。
- 提升方向：
  - 在发帖时通过 Kafka 事件异步为每个粉丝维护一份“个人 Feed 列表”（可以是 Redis List / ZSet），读请求只需从自己的 Feed 列表中取；
  - 这一层已经通过“Kafka Feed 事件 + Redis”完成基础设施铺垫，未来扩展方便。

#### 3.2.3 广场通知流

- 行为事件：点赞、评论、关注、被 @；
- 逻辑链路：
  - `im-square-service` 写入互动记录 & 更新计数 & ZSet 热度后：
    - 通过 HTTP/Kafka 通知 `im-message-service` 或“通知服务”；
  - 通知服务生成一条“广场消息通知”记录（如 `SquareNotification`）；
  - 通过 WebSocket 推送到被通知人；
  - 前端在“我的广场消息”里按时间顺序展示。

这种“行为事件 → 通知流”的模式本质上也是一种 **Feed 流**，只不过它是**带有强主语的系统通知时间线**。

---

## 4. WebSocket / STOMP：长连接消息通道

### 4.1 WebSocket 基础

HTTP 的特点：

- 短连接，请求-响应模式；
- 服务器不能主动向客户端推送，只能被动响应请求（除非使用轮询/长轮询/Server-Sent Events 等技巧）。

WebSocket 解决的问题：

- 建立在 TCP 之上的**持久化全双工连接**；
- 浏览器通过 HTTP 升级握手：
  - `GET /ws HTTP/1.1` + `Upgrade: websocket` 请求；
  - 服务器返回 `101 Switching Protocols` 同意升级；
  - 之后这个 TCP 连接就进入 WebSocket 协议，客户端、服务端都可以主动发消息。

好处：

- 非常适合实时性要求高的场景：IM、在线协作、实时看板、推送等；
- 避免频繁建立 HTTP 连接，节省开销。

### 4.2 STOMP 协议

直接在 WebSocket 上收发原始二进制/文本帧开发复杂度较高，因此 Spring 提供了对 **STOMP** 的支持：

- STOMP 是一个**面向消息的简单文本协议**，提供：
  - `CONNECT` / `SUBSCRIBE` / `SEND` / `DISCONNECT` 等命令；
  - 路径概念：如 `/app/chat`、`/topic/public`、`/user/queue/messages`；
- Spring 框架内置 `@MessageMapping` 注解来处理 STOMP 消息；
- 支持“用户目的地”：`/user/{userId}/queue/xxx`。

在浏览器里通常使用 `SockJS + @stomp/stompjs` 来连接。

### 4.3 本项目中的 WebSocket/STOMP 设计

在 `im-message-service` 中：

- **连接地址**：
  - `ws://{host}/ws?token={JWT}`；
  - 握手拦截器会解析 `token`，校验合法后，把 `userId` 放入 `Principal` 里，后续用于标识连接归属用户。

- **STOMP 路径设计**：
  - 应用前缀：`/app`，例如发送消息用 `/app/chat`；
  - 订阅目的地：
    - 单聊/通知采用 `/user/queue/messages`（每个登录用户都有自己的队列）；
    - 群聊可以用 `/topic/group.{groupId}` 之类的广播通道。

- **消息流转**：
  1. 前端连接：建立 WebSocket + STOMP 连接，订阅 `/user/queue/messages`；
  2. 用户发消息：
     - 前端向 `/app/chat` 发送 STOMP 消息；
  3. 服务器端处理（`@MessageMapping("/chat")`）：
     - 从 `Principal` 拿到 `fromUserId`；
     - 校验双方是否是好友 / 是否被拉黑；
     - 生成 `messageId`，写入 Redis，投递 Kafka 异步落库；
     - 调用 `SimpMessagingTemplate.convertAndSendToUser()` 把消息转发给接收方的 `/queue/messages`；
  4. 接收方前端收到消息后，追加到当前会话窗口中；
  5. 同一通道也用于系统通知的推送（好友申请、广场通知等）。

### 4.4 WebSocket 与 Kafka / Redis 的协同

整体链路可以总结为：

- **入口是 WebSocket**：保证消息实时到达与回显体验；
- **中间是 Kafka**：承接高并发写入、削峰填谷，确保持久化链路可靠；
- **旁路是 Redis**：缓存消息详情、会话列表、热门榜等，优化读写性能；
- **后端 MySQL 落地**：保证消息和广场数据最终一致与可查询。

这一整套组合，是本项目在架构层面最值得对外讲解的一张“技术图”。

---

## 5. 面试/简历中如何讲这几个技术点

你可以按下面这个思路对外讲：

1. **整体链路**：
   - “我们的 IM 系统采用 WebSocket + STOMP 作为长连接消息通道，前端通过 `/app/chat` 发送消息、订阅 `/user/queue/messages` 接收。为了降低数据库压力，我们在服务端只负责做权限校验、写 Redis 缓存以及把消息投递到 Kafka，由后台消费者异步批量写入 MySQL。这样发送 RT 稳定在 10ms 级别，同时提高了系统整体吞吐量。”

2. **Redis ZSet + 广场热门榜**：
   - “广场模块用 Redis ZSet 维护热门帖子榜单，按点赞、收藏、评论行为给帖子打热度分。每次用户操作只是 ZINCRBY 一下分值，热门榜接口直接从 ZSet 里 ZREVRANGE 出前 N 条，再批量查帖子详情即可，几乎不碰数据库，性能和实时性都很好。”

3. **Feed 流 & 事件驱动**：
   - “广场发帖、点赞、评论等行为会同时写入业务表和 Kafka Topic，形成一条行为事件流。后续的广场通知、推荐和报表系统都可以基于这条事件流独立演进，实现模块间解耦和扩展性。”

4. **协同视角**：
   - “整体上是『WebSocket 实时通道 + Kafka 异步流水线 + Redis 热路径缓存 + MySQL 持久化』的架构组合，通过这一套把 IM 和广场场景下的实时性要求、高并发读写和数据一致性做了比较好的平衡。”

有了这些内容，你可以：

- 在简历项目描述里简要提炼（突出异步持久化、ZSet 热门榜、Feed 事件流、WebSocket 设计）；
- 面试时根据面试官兴趣，分别从“为什么要这么做”“如果不用这些技术会怎样”“异常场景如何兜底”等角度展开细节。

---

## 6. 其他可讲的技术点

前面几节主要讲了 ZSet、Kafka、Feed 流和 WebSocket 这一条“主干链路”。在实际项目里，还有几块同样很适合在面试中展开的技术点，可以作为补充：

### 6.1 认证与用户上下文（JWT + Principal）

- **JWT 作为统一认证凭证**：
  - 登录成功后签发 JWT，前端在所有 HTTP 请求头和 WebSocket 连接参数中携带 `token`；
  - 后端通过拦截器统一解析 JWT，验证签名与过期时间，提取 `userId` 等关键信息。
- **UserContext / Principal 统一承载“我是谁”**：
  - HTTP 请求：在过滤器/拦截器里解析完 JWT 后，将 `userId` 写入 `UserContext`（基于 ThreadLocal），业务代码通过静态方法即可拿到当前用户 ID，而不需要层层传参。
  - WebSocket 连接：握手阶段根据 token 构造 `Principal`，`principal.getName()` 直接代表当前连接用户的唯一标识；在 `@MessageMapping` 中通过注入 `Principal` 参数获取发消息人.
- **好处**：
  - 所有服务都围绕“JWT + 用户上下文”这一套做鉴权，接口和 WebSocket 逻辑统一、简单；
  - 对外接口完全无状态，适合后续做水平扩容和网关统一鉴权.

### 6.2 统一错误码与异常处理

- **错误码设计**：
  - 在公共模块 `im-common` 中定义 `ResultCode` 枚举，按业务域划分错误码区间，如用户 1xxx、好友 2xxx、消息 3xxx、广场 7xxx 等；
  - 接口统一返回 `Result<T>`，封装 `code`、`message`、`data` 三个部分.
- **业务异常统一封装**：
  - 业务层通过抛出 `BusinessException(ResultCode.USER_NOT_FOUND)` 或带自定义 message 的构造方法来中断流程；
  - 全局异常处理器捕获后统一转换为 HTTP 200 + 标准 Result JSON，避免异常栈直接泄露给前端.
- **价值**：
  - 错误信息对前端和调用方高度一致，方便前端做统一的错误 toast / 重试逻辑；
  - 线上排障时可以根据错误码快速定位模块和大致问题范围.

### 6.3 会话与未读数设计

- **会话表（Conversation）**：
  - 每个用户针对每个聊天对象（单聊/群聊）都有一条会话记录，包含：会话 ID、会话类型、对端用户/群 ID、最近一条消息摘要、最近消息时间、未读数、是否置顶、是否隐藏等.
- **写入路径中的会话更新**：
  - 当有新消息到达时：
    - 发送方会话：更新最近消息 & 最近时间，未读数一般保持 0；
    - 接收方会话：如果不存在则创建，存在则更新最近消息 & 最近时间并 `unreadCount++`；
  - 这些更新与消息写 Redis、投 Kafka 一起完成，保证每条消息都能反映到会话列表上.
- **清空未读 & 多端同步**：
  - 打开某个会话窗口时，前端调用清空未读接口，将对应会话的 `unreadCount` 置 0；
  - 会话列表接口直接返回新的未读数，用于渲染红点/角标.
- **好处**：
  - 将“消息流”与“会话列表”解耦，按会话维度进行聚合，为展示侧提供非常轻量的接口；
  - 为后续实现置顶、多端同步、漫游消息等能力打基础.

### 6.4 可见性与权限控制（广场 + 社交关系）

- **广场可见性模型**：
  - 帖子定义 `visible_type`（公开、仅好友、不给谁看等）以及可选的排除用户列表；
  - 通过统一的 `isPostVisibleToUser(post, currentUserId)` 方法集中处理可见性判断逻辑：
    - 非作者的情况下检查：
      - 是否仅好友可见且当前用户不是作者好友；
      - 当前用户是否在排除列表中；
      - 未来可扩展黑名单、屏蔽作者等规则.
- **配合好友/黑名单体系**：
  - 用户服务维护好友关系和拉黑列表；
  - 消息服务在发消息前会通过远程调用校验是否被对方拉黑，若被拉黑则拒绝发送；
  - 广场服务在展示帖子时可以联动黑名单逻辑，做内容屏蔽.
- **收益**：
  - 将可见性/权限控制抽象在统一位置，便于扩展和审计；
  - 面试时可以从“权限模型设计”和“业务规则可配置性”的角度展开说明.

### 6.5 缓存策略与降级思路

- **多层缓存设计**：
  - 聊天消息：近期消息和消息详情缓存在 Redis，历史消息翻页时回退 MySQL；
  - 会话列表：可以部分缓存，减少频繁的聚合查询；
  - 广场公共时间线：按页缓存帖子 ID 列表，命中时只需批量查详情；
  - 热门榜：完全由 Redis ZSet 驱动，只在最终聚合帖子详情时访问数据库.
- **缓存失效与精细化清理**：
  - 针对发帖、点赞、评论等操作，只清理相关页的缓存 key 或对应用户的缓存，避免“大面积全清”；
  - 对于请求带多条件搜索参数的情况，直接绕过缓存走实时查询，避免缓存维度爆炸.
- **降级策略**：
  - Redis 不可用时，热门榜和时间线回退到 MySQL 排序 + 分页查询；
  - Kafka 不可用时可以切换到“同步直写 DB”的应急路径（或通过开关快速关闭某些非关键功能），保证核心 IM 能力可用.

### 6.6 前端工程化与交互细节

- **前端技术栈**：
  - Vue 3 + TypeScript + Vite，组件库使用 Element Plus；
  - 使用 Pinia 维护全局用户信息、会话列表、未读数等状态，避免多页面之间数据不一致.
- **接口封装与模块划分**：
  - 将后端接口按领域拆分到 `src/api/user.js`、`src/api/message.js`、`src/api/square.js` 等文件中；
  - 使用 Axios 实例统一配置 baseURL、超时和拦截器（如自动携带 JWT、统一处理登录过期等）.
- **广场筛选栏的交互设计**：
  - 在 `Square.vue` 中将搜索、标签、多条件筛选（有图/有视频/可见范围）和排序封装成一个上方面板，使用 Element Plus 的表单控件和布局组件实现紧凑的两行布局；
  - 任何一个筛选条件改变时，自动触发重新加载列表，前端只负责组织参数，具体查询逻辑在后端统一处理.
- **价值**：
  - 体现前后端契约清晰、关注点分离：前端聚焦交互体验和参数组织，后端负责搜索/排序/权限控制；
  - 面试中可以展示自己对“工程化”和“用户体验”的关注，不仅仅是堆技术名词.

---

有了这一节，你在对外讲这个项目时，除了那条「WebSocket + Kafka + Redis + MySQL」主干链路，还可以从鉴权、错误码、会话模型、权限控制、缓存策略和前端工程化等多个角度选取 2~3 个点展开，避免讲解内容过于单一.
